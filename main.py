#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
3-Stage LTV Prediction Pipeline

This script trains a three-stage model to predict user LTV.
- Stage 1 (Binary: Payer vs Non-Payer): CatBoost + LightGBM + XGBoost (hard voting).
- Stage 2 (Binary: Whale vs Non-Whale): CatBoost + LightGBM + TabPFN (hard voting).
- Stage 3 (Regression on Payers): Two-headed CatBoost + LightGBM + TabPFN Regressors (non-whale/whale).

The script is designed to be executed from the command line, with arguments to
specify the execution stage and random seed.
"""

import os
import sys
import math
import random
import warnings
import argparse
from pathlib import Path
from typing import List, Tuple, Dict
import matplotlib.pyplot as plt
import joblib
import logging
from datetime import datetime
import uuid
import traceback
import json

import numpy as np
import pandas as pd

from catboost import CatBoostClassifier, CatBoostRegressor, Pool
import lightgbm as lgb
import xgboost as xgb

import optuna
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner
from optuna.integration import XGBoostPruningCallback, LightGBMPruningCallback

import torch

try:
    from tabpfn import TabPFNClassifier, TabPFNRegressor
    _HAS_TABPFN = True
except Exception as e:
    _HAS_TABPFN = False
    warnings.warn(f"TabPFN import failed: {e}. Stage2/3 will skip TabPFN.")

from sklearn.metrics import (
    f1_score, precision_score, recall_score, confusion_matrix, mean_absolute_error, mean_squared_error,roc_auc_score
)

# ------Test mode------
TEST_FRACTION = 0.10  # test_modeì—ì„œ ì‚¬ìš©í•  ìƒ˜í”Œë§ ë¹„ìœ¨

# =====================================================================================
# ---- 1. CONFIGURATION & PATHS
# =====================================================================================

# main.pyê°€ ìžˆëŠ” í´ë” (ver1_stage1)
SCRIPT_DIR = Path(__file__).resolve().parent

# ë°ì´í„° í´ë” ê²½ë¡œ (ver1_stage1ì—ì„œ í•œ ë‹¨ê³„ ìœ„ë¡œ ì˜¬ë¼ê°€ Data í´ë”ë¡œ ì´ë™)
DATA_DIR = SCRIPT_DIR.parent / "Data"

# ê²°ê³¼ë¬¼ ì €ìž¥ ê²½ë¡œ
# ver1_stage1 í´ë” ë‚´ì— ìƒì„±ë©ë‹ˆë‹¤.
RESULTS_DIR = SCRIPT_DIR / "seed_results_stage1"
RESULTS_DIR.mkdir(exist_ok=True) 

# ì´ ë¶€ë¶„ì€ DATA_DIRì´ ì˜¬ë°”ë¥´ê²Œ ì •ì˜ë˜ë©´ ìžë™ìœ¼ë¡œ ìž˜ ìž‘ë™í•©ë‹ˆë‹¤.
DATA_PATHS = {
    "train": str(DATA_DIR / "train_df_5days.parquet"),
    "val":   str(DATA_DIR / "val_df_5days.parquet"),
    "test":  str(DATA_DIR / "test_df_5days.parquet"),
    "train_robust": str(DATA_DIR / "train_df_5days_robust.parquet"),
    "val_robust":   str(DATA_DIR / "val_df_5days_robust.parquet"),
    "test_robust":  str(DATA_DIR / "test_df_5days_robust.parquet"),
}

# --- Global Settings ---
DEFAULT_SEED = 2025

#################### ë³¸ì¸ ì‹œë“œë¡œ ìˆ˜ì •ìˆ˜ì •!!!!!!!!!!!1##################
SEEDS = list(range(2021, 2024))  # 10 seeds: 2021..2030
#################### ë³¸ì¸ ì‹œë“œë¡œ ìˆ˜ì •ìˆ˜ì •!!!!!!!!!!!1##################

TARGET_COL = "PAY_AMT_SUM"
ID_COL = "PLAYERID"
WHALE_Q = 0.95  # top 5% among TRAIN payers

# --- Model & Tuning Settings ---
CUT_STEP = 0.01
DELTA_AROUND = 0.15
USE_OPTUNA = True

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# CAT_TASK_PARAMS = {} # GPU ì‚¬ìš© ì‹œ {"task_type": "GPU"}
CAT_TASK_PARAMS = {"task_type": "GPU"} if DEVICE == "cuda" else {}

# --- Stage 1 Fixed Hyperparameters ---
RUN_LR = 0.05
LGBM_FIXED = dict(
    objective="binary", learning_rate=RUN_LR, subsample=0.1, min_child_samples=20,
    reg_alpha=0.1, reg_lambda=0.1, verbosity=-1,
)
LGBM_MAX_DEPTH_RANGE = (7, 14)
LGBM_N_EST_RANGE     = (800, 1600)

XGB_FIXED = dict(
    objective="binary:logistic", eval_metric="auc", learning_rate=RUN_LR, subsample=0.1,
    reg_alpha=0.1, reg_lambda=0.1, tree_method="hist", max_bin=256,
)
XGB_MAX_DEPTH_RANGE = (7, 14)
XGB_N_EST_RANGE     = (800, 1600)

CAT_FIXED = dict(
    loss_function="Logloss", eval_metric="F1", learning_rate=RUN_LR, verbose=0,
)
CAT_DEPTH_RANGE = (7, 14)
CAT_ITER_RANGE  = (800, 1600)

OPTUNA_TRIALS = {
    "stage1": 50,
    "stage2": 30,
    "stage3_nw": 30,
    "stage3_w": 30,
}

OPTUNA_SEED = DEFAULT_SEED

# =====================================================================================
# ---- 2. UTILITY FUNCTIONS (Original notebook code)
# =====================================================================================

def _select_params(model_name: str, params: dict) -> dict:
    """
    CSV ê¸¸ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ëª¨ë¸ë³„ í•µì‹¬ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ë ¤ ë°˜í™˜.
    """
    if model_name == "lgbm":
        keep = [
            "n_estimators","learning_rate","num_leaves","max_depth",
            "min_child_samples","subsample","colsample_bytree","reg_alpha","reg_lambda",
            "random_state"
        ]
    elif model_name == "xgb":
        keep = [
            "n_estimators","learning_rate","max_depth","min_child_weight",
            "subsample","colsample_bytree","reg_alpha","reg_lambda","scale_pos_weight",
            "random_state","n_jobs"
        ]
    else:  # cat
        keep = [
            "depth","iterations","learning_rate","l2_leaf_reg",
            "bagging_temperature","random_seed","class_weights"
        ]
    return {k: params.get(k) for k in keep if k in params}


# --- TabPFN Helper ---
import inspect

def _construct_tabpfn(cls, device: str, seed: int, n_ens: int):
    try:
        sig = inspect.signature(cls.__init__)
        kw = {}
        # ê³µí†µì ìœ¼ë¡œ ì‹œë„ ê°€ëŠ¥í•œ ì¸ìžë§Œ ì„ ë³„
        if "device" in sig.parameters:
            kw["device"] = device
        # ì•™ìƒë¸” í¬ê¸° ì´ë¦„ í˜¸í™˜
        if "N_ensemble_configurations" in sig.parameters:
            kw["N_ensemble_configurations"] = n_ens
        elif "n_estimators" in sig.parameters:
            kw["n_estimators"] = n_ens
        # ì‹œë“œ ì´ë¦„ í˜¸í™˜
        if "seed" in sig.parameters:
            kw["seed"] = seed
        elif "random_state" in sig.parameters:
            kw["random_state"] = seed
        # ìœ„ ì¸ìžë“¤ì´ í•˜ë‚˜ë„ ì•ˆ ë§žì•„ë„ ìµœì†Œí•œ ê¸°ë³¸ ìƒì„±ì€ ì‹œë„
        return cls(**kw) if kw else cls()
    except Exception:
        # ìµœí›„ì˜ ë³´ë£¨: ì•„ë¬´ ì¸ìž ì—†ì´ ìƒì„±
        return cls()

def _make_tabpfn_classifier(device: str, seed: int, n_ens: int = 16):
    if not _HAS_TABPFN:
        return None
    return _construct_tabpfn(TabPFNClassifier, device=device, seed=seed, n_ens=n_ens)

def _make_tabpfn_regressor(device: str, seed: int, n_ens: int = 16):
    if not _HAS_TABPFN:
        return None
    return _construct_tabpfn(TabPFNRegressor, device=device, seed=seed, n_ens=n_ens)

# --- Metrics & Preprocessing ---
def smape(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=float)
    y_pred = np.asarray(y_pred, dtype=float)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2
    diff = np.abs(y_true - y_pred) / np.where(denom == 0, 1, denom)
    diff[denom == 0] = 0.0
    return float(np.mean(diff) * 100)

class OrdinalCategoryEncoder:
    def __init__(self):
        self.maps: Dict[str, Dict] = {}
        self.cols: List[str] = []

    def fit(self, df: pd.DataFrame, cat_cols: List[str]):
        self.cols = list(cat_cols)
        for c in self.cols:
            # ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ì•ˆì „í•˜ê²Œ í™•ë³´
            cats = pd.Series(df[c].astype("category").cat.categories)
            self.maps[c] = {cat: i for i, cat in enumerate(cats)}
        return self

    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        out = df.copy()
        for c in self.cols:
            if c in out.columns:
                mapping = self.maps[c]
                # âš ï¸ Categorical â†’ objectë¡œ í’€ê³  dict.getìœ¼ë¡œ ì•ˆì „ ë§¤í•‘
                s = out[c].astype(object)
                out[c] = s.apply(lambda v: mapping.get(v, -1)).astype(np.int32)
        return out

class XGBCompat:
    """
    xgboost ë²„ì „ ë¬´ê´€í•˜ê²Œ early stopping + predict_proba ì œê³µ.
    ë‚´ë¶€ì ìœ¼ë¡œ sklearn APIê°€ ì•„ë‹ˆë¼ xgb.train(DMatrix) ì‚¬ìš©.
    """
    def __init__(self, **params):
        self.params = params.copy()
        self.booster_ = None
        self.best_ntree_limit_ = None
        self._num_boost_round = int(self.params.pop("n_estimators", 100))

    def _to_train_params(self):
        p = self.params.copy()
        # ì´ë¦„ ë§¤í•‘
        if "random_state" in p and "seed" not in p:
            p["seed"] = p.pop("random_state")
        if "n_jobs" in p and "nthread" not in p:
            p["nthread"] = p.pop("n_jobs")
        # ì•ˆì „ ê¸°ë³¸ê°’
        p.setdefault("objective", "binary:logistic")
        p.setdefault("eval_metric", p.get("eval_metric", "auc"))
        return p

    def fit(self, X_tr, y_tr, X_va, y_va, early_stopping_rounds=200, verbose_eval=False):
        import xgboost as xgb
        dtr = xgb.DMatrix(X_tr, label=y_tr)
        dva = xgb.DMatrix(X_va, label=y_va)
        train_params = self._to_train_params()
        self.booster_ = xgb.train(
            train_params,
            dtr,
            num_boost_round=self._num_boost_round,
            evals=[(dtr, "train"), (dva, "valid")],
            early_stopping_rounds=early_stopping_rounds,
            verbose_eval=verbose_eval,
        )
        # best_ntree_limitì€ ë²„ì „ì— ë”°ë¼ ì—†ì„ ìˆ˜ ìžˆìŒ
        self.best_ntree_limit_ = getattr(self.booster_, "best_ntree_limit", None)
        return self

    def predict_proba(self, X):
        import xgboost as xgb
        d = xgb.DMatrix(X)
        if self.best_ntree_limit_:
            p1 = self.booster_.predict(d, ntree_limit=self.best_ntree_limit_)
        else:
            p1 = self.booster_.predict(d)
        p1 = np.asarray(p1, dtype=float).reshape(-1)
        p0 = 1.0 - p1
        return np.column_stack([p0, p1])

# --- Data Loading ---
action_trash_list = ['ê¸¸ë“œ_í•˜ìš°ìŠ¤ ëŒ€ì—¬', 'ìºì‹œ ìƒì _ì•„ì´í…œ ì‚­ì œ', 'ê¸¸ë“œ_ê°€ìž… ì‹ ì²­', 'ê³„ì •_ë¡œê·¸ì¸', 'í´ëž˜ìŠ¤_ìž ê¸ˆ',
                     'ê¸¸ë“œ_ì„¤ì • ë³€ê²½', 'ì„±ìž¥_ë ˆë²¨ ë‹¤ìš´', 'ì„±ìž¥_ìŠ¤í‚¬ ìŠµë“', 'ê·¸ë¡œì•„_ì†Œí™˜ í™•ì • ëŒ€ê¸° ë³€ê²½', 'ì•„ì´í…œ ì»¬ë ‰ì…˜_ì¶”ê°€',
                     'ê·¸ë¡œì•„_ì†Œí™˜', 'íƒˆê²ƒ_ìŠ¤í‚¬ ì„¤ì •', 'í€˜ìŠ¤íŠ¸_ë³´ìƒ ë¯¸ë¦¬ë³´ê¸° ì‚­ì œ', 'ìºì‹œ ìƒì _ì•„ì´í…œ ì¶”ê°€', 'ê¸¸ë“œ_ìƒì„±', 'ì œìž‘_ì œìž‘',
                     'í´ëž˜ìŠ¤_ì†Œí™˜ í™•ì • ëŒ€ê¸° ìƒì„±', 'ê³„ì •_ë¡œê·¸ì•„ì›ƒ', 'ê¸¸ë“œ_ì ëŒ€ ë“±ë¡ ì·¨ì†Œ', 'ê¸¸ë“œ_ë“±ê¸‰', 'ê¸¸ë“œ_ë™ë§¹ ì‹ ì²­ ì·¨ì†Œ', 'ë³´ìŠ¤ì „_í•„ë“œ ë³´ìŠ¤',
                     'ê¸¸ë“œ_ë™ë§¹ ì‹ ì²­', 'íƒˆê²ƒ_ì¶”ê°€', 'íƒˆê²ƒ_ì†Œí™˜ í™•ì • ëŒ€ê¸° ë³€ê²½', 'í€˜ìŠ¤íŠ¸_í¬ê¸°', 'ê·¸ë¡œì•„_ì†Œí™˜ í™•ì • ëŒ€ê¸° ìƒì„±', 'ì„±ìž¥_ë ˆë²¨ ì—…',
                     'ìºì‹œ ìƒì _ì›”ë“œ ì¶”ê°€', 'ì‚¬ë§ ë¶ˆì´ìµ_ê²½í—˜ì¹˜', 'ìºì‹œ ìƒì _ìºì‹œ ìƒì ì—ì„œ ìž¬í™”ë¡œ êµ¬ë§¤', 'í€˜ìŠ¤íŠ¸_ë³´ìƒ ë¯¸ë¦¬ë³´ê¸°', 'ìºë¦­í„°_ìƒì„±',
                     'í´ëž˜ìŠ¤_ì†Œí™˜ í™•ì • ëŒ€ê¸° ë³€ê²½', 'ê¸¸ë“œ_ì ëŒ€ ë“±ë¡', 'ë˜ì ¼_ì¶©ì „', 'ìŠ¤íƒ¯_ì„¤ì •', 'ê¸°ë¯¹_ë“±ì§', 'í´ëž˜ìŠ¤_ì†Œí™˜ í™•ì • ëŒ€ê¸° ì‚­ì œ', 'ê·¸ë¡œì•„_ì†Œí™˜ í™•ì • ëŒ€ê¸° ì‚­ì œ',
                     'ì„±ìž¥_ìƒíƒœ ë³€í™” ìŠµë“', 'ì„±ìž¥_ì£½ìŒ', 'ì œìž‘_ì¶”ê°€', 'í€˜ìŠ¤íŠ¸_ì˜ë¢° ê°±ì‹ ', 'ê¸¸ë“œ_ì§€ì›ìž ì œê±°', 'ìºì‹œ ìƒì _ìºë¦­í„° ì¶”ê°€', 'ê¸¸ë“œ_ë™ë§¹ íŒŒê¸°', 'ì›Œí”„_ê°±ì‹ ',
                     'ì›Œí”„_ì‚­ì œ', 'í´ëž˜ìŠ¤_ì¶”ê°€', 'ê¸¸ë“œ_ê°€ìž…', 'ê¸¸ë“œ_ë™ë§¹ ì‹ ì²­ í™•ì¸', 'ë³´ìŠ¤ì „_ì›”ë“œ ë³´ìŠ¤', 'í€˜ìŠ¤íŠ¸_ì™„ë£Œ', 'ê¸¸ë“œ_í•´ì²´', 'íƒˆê²ƒ_ìž ê¸ˆ', 'ìºì‹œ ìƒì _ê³„ì • ì¶”ê°€',
                     'ì›Œí”„_ìƒì„±', 'ì›Œí”„_ìˆœê°„ì´ë™ ì‚¬ìš©', 'ì„±ìž¥_ê²½í—˜ì¹˜ ì†ì‹¤', 'í€˜ìŠ¤íŠ¸_ì˜ë¢°', 'í€˜ìŠ¤íŠ¸_ìˆ˜ë½', 'íƒˆê²ƒ_ë“±ë¡', 'í€˜ìŠ¤íŠ¸_ìˆ˜í–‰', 'ê¸¸ë“œ_ê²½í—˜ì¹˜ íšë“', 'ê·¸ë¡œì•„_ìž ê¸ˆ',
                     'ìºì‹œ ìƒì _êµ¬ë§¤ ë‚˜ì´ ë³€ê²½', 'ê¸¸ë“œ_ë™ë§¹ ì‹ ì²­ ê±°ì ˆ', 'íƒˆê²ƒ_ì†Œí™˜ í™•ì • ëŒ€ê¸° ìƒì„±', 'í´ëž˜ìŠ¤_ë³€ê²½', 'íƒˆê²ƒ_ì†Œí™˜ í™•ì • ëŒ€ê¸° ì‚­ì œ', 'ê¸¸ë“œ_íƒˆí‡´', 'ì‚¬ë§ ë¶ˆì´ìµ_ì•„ì´í…œ',
                     'ê¸¸ë“œ_ì¶œì„', 'ê·¸ë¡œì•„_ì¶”ê°€']

action_list = ['PLAYERID','ê³„ì •', 'ê·¸ë¡œì•„', 'ê¸°ë¯¹', 'ê¸¸ë“œ', 'ë˜ì ¼', 'ë³´ìŠ¤ì „',
               'ì‚¬ë§ ë¶ˆì´ìµ', 'ì„±ìž¥', 'ìŠ¤íƒ¯', 'ì•„ì´í…œ ì»¬ë ‰ì…˜', 'ì›Œí”„', 'ì œìž‘',
               'ìºë¦­í„°', 'ìºì‹œ ìƒì ', 'í€˜ìŠ¤íŠ¸', 'í´ëž˜ìŠ¤', 'íƒˆê²ƒ']

def load_pre_split(is_test_mode: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    # base
    df_train = pd.read_parquet(DATA_PATHS["train"], engine="pyarrow")
    df_val   = pd.read_parquet(DATA_PATHS["val"],   engine="pyarrow")
    df_test  = pd.read_parquet(DATA_PATHS["test"],  engine="pyarrow")
    # robust (ìŠ¬ë¦¼ ì»¬ëŸ¼ë§Œ ìœ ì§€)
    rb_train = pd.read_parquet(DATA_PATHS["train_robust"], engine="pyarrow")
    rb_val   = pd.read_parquet(DATA_PATHS["val_robust"],   engine="pyarrow")
    rb_test  = pd.read_parquet(DATA_PATHS["test_robust"],  engine="pyarrow")

    base_train = df_train.drop(columns=action_trash_list, errors="ignore")
    base_val   = df_val.drop(columns=action_trash_list, errors="ignore")
    base_test  = df_test.drop(columns=action_trash_list, errors="ignore")

    rb_train = rb_train[action_list]
    rb_val   = rb_val[action_list]
    rb_test  = rb_test[action_list]

    train = base_train.merge(rb_train, on="PLAYERID", how="left")
    val   = base_val.merge(rb_val,     on="PLAYERID", how="left")
    test  = base_test.merge(rb_test,   on="PLAYERID", how="left")

    # NAT_CD ì¹´í…Œê³ ë¦¬ ì •í•©ì„±(ìžˆìœ¼ë©´)
    if "NAT_CD" in train.columns:
        train["NAT_CD"] = train["NAT_CD"].astype("category")
        cats = train["NAT_CD"].cat.categories
        if "NAT_CD" in val.columns:
            val["NAT_CD"] = pd.Categorical(val["NAT_CD"], categories=cats)
        if "NAT_CD" in test.columns:
            test["NAT_CD"] = pd.Categorical(test["NAT_CD"], categories=cats)

    if is_test_mode:
        logging.info("âš¡ Test mode enabled. Sampling fixed counts (train=2000, val=1000, test=1000).")
        train = train.sample(n=min(2000, len(train)), random_state=DEFAULT_SEED)
        val   = val.sample(n=min(1000, len(val)),   random_state=DEFAULT_SEED)
        test  = test.sample(n=min(1000, len(test)), random_state=DEFAULT_SEED)

    def _ratio(df):
        return float((df[TARGET_COL] > 0).mean()) if len(df) > 0 else 0.0
    logging.info(f"ðŸ”Ž Loaded pre-split | payer ratio â€” train:{_ratio(train):.4f}  val:{_ratio(val):.4f}  test:{_ratio(test):.4f}")
    return train, val, test

def _sanitize_cols(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out.columns = out.columns.astype(str).str.replace(r"\s+", "_", regex=True)
    return out

def build_features(df: pd.DataFrame, target_col: str, drop_cols: List[str]):
    cols = [c for c in df.columns if c not in drop_cols]
    cat_cols = [c for c in cols if df[c].dtype == "object" or str(df[c].dtype).startswith("category")]
    return df[cols].copy(), cols, cat_cols

def fit_imputer(train_df: pd.DataFrame):
    num_cols = [c for c in train_df.columns if train_df[c].dtype != 'object' and not str(train_df[c].dtype).startswith('category')]
    med = train_df[num_cols].median(numeric_only=True)
    return num_cols, med

def apply_imputer(df: pd.DataFrame, num_cols: list[str], med: pd.Series):
    df = df.copy()
    df[num_cols] = df[num_cols].fillna(med)
    # OrdinalCategoryEncoderë¡œ ë²”ì£¼ëŠ” ì´ë¯¸ ì •ìˆ˜í™”ë¨(ë¯¸ë“±ë¡ì€ -1), í˜¹ì‹œ ë‚¨ì€ NaNì´ ìžˆìœ¼ë©´ 0ìœ¼ë¡œ:
    df = df.fillna(0)
    return df

# =====================================================================================
# ---- 3. STAGE-SPECIFIC TRAINING LOGIC (All functions from notebook)
# =====================================================================================
# ------------------------
# Cutoff search
# ------------------------

def _search_cutoff_grid(y_true, proba, center: float, delta: float = DELTA_AROUND, step: float = CUT_STEP, metric: str = "f1"):
    lo = max(0.0, center - delta)
    hi = min(1.0, center + delta)
    grid = np.arange(lo, hi + 1e-9, step)
    y_true = np.asarray(y_true).astype(int)
    scores = {}
    best_t, best_s = 0.5, -1.0
    for t in grid:
        y_hat = (proba >= t).astype(int)
        if metric == "f1":
            s = f1_score(y_true, y_hat) if (y_hat.sum()>0 and y_true.sum()>0) else 0.0
        elif metric == "balanced_acc":
            tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()
            tpr = tp / (tp + fn + 1e-12)
            tnr = tn / (tn + fp + 1e-12)
            s = 0.5*(tpr+tnr)
        else:
            s = f1_score(y_true, y_hat)
        scores[float(t)] = float(s)
        if s > best_s:
            best_t, best_s = float(t), float(s)
    return best_t, scores


def tune_cutoff(y_true, proba, strategy: str, train_pos_prior: float, metric: str = "f1"):
    if strategy == "prior":
        center = float(np.clip(train_pos_prior, 0.05, 0.95))
    elif strategy == "reweight":
        center = 0.5
    else:
        center = 0.5
    return _search_cutoff_grid(y_true, proba, center=center, metric=metric)

# ------------------------
# Optuna - classification
# ------------------------

def _tune_lgbm_cls(X_tr, y_tr, X_va, y_va, stage_key: str, strategy: str, train_pos_prior: float, size="large"):
    # size: "large"(stage1) | "small"(stage2)
    n_trials = OPTUNA_TRIALS[stage_key]
    if size == "large":
        bounds = dict(
            n_estimators=(800, 4000),
            learning_rate=(0.01, 0.1),
            num_leaves=(63, 255),
            max_depth=(-1, 14),
            min_child_samples=(20, 200),
            subsample=(0.6, 1.0),
            colsample_bytree=(0.6, 1.0),
            reg_alpha=(0.0, 5.0),
            reg_lambda=(0.0, 10.0),
        )
    else:  # small (stage2)
        bounds = dict(
            n_estimators=(500, 2500),
            learning_rate=(0.01, 0.15),
            num_leaves=(31, 127),
            max_depth=(-1, 10),
            min_child_samples=(10, 200),
            subsample=(0.6, 1.0),
            colsample_bytree=(0.6, 1.0),
            reg_alpha=(0.0, 8.0),
            reg_lambda=(0.0, 15.0),
        )

    def objective(trial):
        params = dict(
            n_estimators=trial.suggest_int("n_estimators", *bounds["n_estimators"]),
            learning_rate=trial.suggest_float("learning_rate", *bounds["learning_rate"], log=True),
            num_leaves=trial.suggest_int("num_leaves", *bounds["num_leaves"]),
            max_depth=trial.suggest_int("max_depth", *bounds["max_depth"]),
            min_child_samples=trial.suggest_int("min_child_samples", *bounds["min_child_samples"]),
            subsample=trial.suggest_float("subsample", *bounds["subsample"]),
            colsample_bytree=trial.suggest_float("colsample_bytree", *bounds["colsample_bytree"]),
            reg_alpha=trial.suggest_float("reg_alpha", *bounds["reg_alpha"]),
            reg_lambda=trial.suggest_float("reg_lambda", *bounds["reg_lambda"]),
            objective="binary",
            random_state=SEED,
            n_jobs=max(1, (os.cpu_count() or 8)//4),
            verbosity=-1,
            force_row_wise=True,
        )
        model = lgb.LGBMClassifier(**params)
        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric="auc",
                  callbacks=[lgb.early_stopping(200, verbose=False)])
        proba = model.predict_proba(X_va)[:,1]
        t_opt, _ = tune_cutoff(y_va, proba, strategy=strategy, train_pos_prior=train_pos_prior)
        pred = (proba >= t_opt).astype(int)
        return f1_score(y_va, pred)

    study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=OPTUNA_SEED))
    study.optimize(objective, n_trials=n_trials)
    best = study.best_params
    best.update(dict(objective="binary", random_state=SEED, n_jobs=max(1, (os.cpu_count() or 8)//4)))
    # ðŸ”‡ ë¶ˆí•„ìš” ë¡œê·¸/ì˜¤ë²„í—¤ë“œ ì¶•ì†Œ
    best.update({"verbosity": -1,         # info ë¡œê·¸ ë„ê¸°
                 "force_row_wise": True,  # "Auto-choosing row-wise..." ì•ˆë‚´ë¬¸ ì œê±° + ê³ ì •
    })
    return best


def _tune_xgb_cls(X_tr, y_tr, X_va, y_va, stage_key: str, strategy: str, train_pos_prior: float,
                  scale_pos_weight=1.0, size="large"):
    n_trials = OPTUNA_TRIALS[stage_key]
    if size == "large":
        bounds = dict(
            n_estimators=(800, 4000),
            learning_rate=(0.01, 0.2),
            max_depth=(4, 10),
            min_child_weight=(1.0, 10.0),
            subsample=(0.6, 1.0),
            colsample_bytree=(0.6, 1.0),
            reg_alpha=(0.0, 5.0),
            reg_lambda=(0.0, 10.0),
        )
    else:
        bounds = dict(
            n_estimators=(500, 2500),
            learning_rate=(0.01, 0.2),
            max_depth=(3, 8),
            min_child_weight=(1.0, 12.0),
            subsample=(0.6, 1.0),
            colsample_bytree=(0.6, 1.0),
            reg_alpha=(0.0, 8.0),
            reg_lambda=(0.0, 15.0),
        )

    def objective(trial):
        params = dict(
            n_estimators=trial.suggest_int("n_estimators", *bounds["n_estimators"]),
            learning_rate=trial.suggest_float("learning_rate", *bounds["learning_rate"], log=True),
            max_depth=trial.suggest_int("max_depth", *bounds["max_depth"]),
            min_child_weight=trial.suggest_float("min_child_weight", *bounds["min_child_weight"]),
            subsample=trial.suggest_float("subsample", *bounds["subsample"]),
            colsample_bytree=trial.suggest_float("colsample_bytree", *bounds["colsample_bytree"]),
            reg_alpha=trial.suggest_float("reg_alpha", *bounds["reg_alpha"]),
            reg_lambda=trial.suggest_float("reg_lambda", *bounds["reg_lambda"]),
            objective="binary:logistic",
            random_state=SEED,
            n_jobs=max(1, (os.cpu_count() or 8)//4),
            tree_method="hist",
            max_bin=256,
            scale_pos_weight=scale_pos_weight,
            eval_metric="auc",
        )
        model = XGBCompat(**params)
        model.fit(X_tr, y_tr, X_va, y_va, early_stopping_rounds=200, verbose_eval=False)
        proba = model.predict_proba(X_va)[:, 1]
        t_opt, _ = tune_cutoff(y_va, proba, strategy=strategy, train_pos_prior=train_pos_prior)
        pred = (proba >= t_opt).astype(int)
        return f1_score(y_va, pred)

    study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=OPTUNA_SEED))
    study.optimize(objective, n_trials=n_trials)
    best = study.best_params
    best.update(dict(objective="binary:logistic", random_state=SEED, n_jobs=max(1, (os.cpu_count() or 8)//4),
                     tree_method="hist", max_bin=256, scale_pos_weight=scale_pos_weight, eval_metric="auc",))
    return best


def _tune_cat_cls(X_tr, y_tr, X_va, y_va, cat_cols_idx, stage_key: str, strategy: str,
                  train_pos_prior: float, class_weights=None, size="large"):
    n_trials = OPTUNA_TRIALS[stage_key]
    bounds = dict(iterations=(800, 4000), depth=(5, 10)) if size=="large" else dict(iterations=(600, 2500), depth=(4, 8))

    def objective(trial):
        params = dict(
            depth=trial.suggest_int("depth", *bounds["depth"]),
            learning_rate=trial.suggest_float("learning_rate", 0.02, 0.2, log=True),
            l2_leaf_reg=trial.suggest_float("l2_leaf_reg", 1.0, 12.0),
            bagging_temperature=trial.suggest_float("bagging_temperature", 0.0, 3.0),
            random_strength=trial.suggest_float("random_strength", 0.0, 2.0),
            iterations=trial.suggest_int("iterations", *bounds["iterations"]),
            loss_function="Logloss",
            eval_metric="F1",
            random_seed=SEED,
            verbose=0,
        )
        if class_weights is not None:
            params["class_weights"] = class_weights

        model = CatBoostClassifier(**params, **CAT_TASK_PARAMS, od_type="Iter", od_wait=200)
        pool_tr = Pool(X_tr, y_tr, cat_features=cat_cols_idx or None)
        pool_va = Pool(X_va, y_va, cat_features=cat_cols_idx or None)

        model.fit(pool_tr, eval_set=pool_va, use_best_model=True, verbose=False)
        proba = model.predict_proba(pool_va)[:, 1]
        t_opt, _ = tune_cutoff(y_va, proba, strategy=strategy, train_pos_prior=train_pos_prior)
        return f1_score(y_va, (proba >= t_opt).astype(int))

    study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=OPTUNA_SEED))
    study.optimize(objective, n_trials=n_trials)
    best = study.best_params
    best.update(dict(loss_function="Logloss", eval_metric="F1", random_seed=SEED, verbose=0))
    if class_weights is not None:
        best["class_weights"] = class_weights
    return best

def _tune_lgbm_md_ne_fixedgrid(X_tr, y_tr, X_va, y_va, stage_key: str, strategy: str, train_pos_prior: float,
                               lgb_train=None, lgb_valid=None):
    assert lgb_train is not None and lgb_valid is not None, "Pass prebuilt lgb.Dataset"
    n_trials = OPTUNA_TRIALS[stage_key]

    study = optuna.create_study(direction="maximize",
                                sampler=TPESampler(seed=DEFAULT_SEED),
                                pruner=MedianPruner(n_warmup_steps=5, n_min_trials=3))

    def objective(trial):
        # âœ… ê³ ì •ê°’ + ì†ë„ ì˜µì…˜
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'learning_rate': 0.05,
            'subsample': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'min_child_samples': 20,
            'verbosity': -1,            # ðŸ”‡
            'random_state': DEFAULT_SEED,
            'n_jobs': -1,
            'force_row_wise': True,     # âš¡
        }

        # âœ… íŠœë‹ ê°’
        params['max_depth'] = trial.suggest_int("max_depth", *LGBM_MAX_DEPTH_RANGE)
        params['n_estimators'] = trial.suggest_int("n_estimators", *LGBM_N_EST_RANGE)
        params['colsample_bytree'] = trial.suggest_categorical("colsample_bytree", [0.1, 0.3, 0.5])

        # ðŸ”ª Optuna pruner + â±ï¸ ì¡°ê¸° ì¢…ë£Œ
        pruning_callback = LightGBMPruningCallback(trial, "auc", valid_name="valid")
        early_stop_cb = lgb.early_stopping(200, verbose=False)
        log_cb = lgb.log_evaluation(0)

        # âœ… n_estimatorsë¥¼ num_boost_roundë¡œ ë°˜ì˜
        num_boost_round = int(params.pop('n_estimators'))

        booster = lgb.train(
            params,
            lgb_train,
            num_boost_round=num_boost_round,
            valid_sets=[lgb_valid],
            valid_names=["valid"],
            callbacks=[pruning_callback, early_stop_cb, log_cb],
        )

        proba = booster.predict(X_va, num_iteration=booster.best_iteration)
        t_opt, _ = tune_cutoff(y_va, proba, strategy=strategy, train_pos_prior=train_pos_prior)
        return f1_score(y_va, (proba >= t_opt).astype(int))

    study.optimize(objective, n_trials=n_trials)
    best_params = study.best_params
    best_params.update({
        'objective': 'binary',
        'learning_rate': 0.05,
        'subsample': 0.1,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'min_child_samples': 20,
        'verbosity': -1,
        'random_state': DEFAULT_SEED,
        'n_jobs': -1,
        'force_row_wise': True,
    })
    return best_params


def _tune_xgb_md_ne_fixedgrid(X_tr, y_tr, X_va, y_va, stage_key: str, strategy: str, train_pos_prior: float,
                              scale_pos_weight=1.0, dtr=None, dva=None):
    assert dtr is not None and dva is not None, "Pass prebuilt DMatrix"
    n_trials = OPTUNA_TRIALS[stage_key]

    study = optuna.create_study(direction="maximize",
                                sampler=TPESampler(seed=DEFAULT_SEED),
                                pruner=MedianPruner(n_warmup_steps=5, n_min_trials=3))

    def objective(trial):
        params = {
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'learning_rate': 0.05,
            'subsample': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'tree_method': 'hist',    # GPU ì“°ë©´ 'gpu_hist'ë¡œ êµì²´ ê°€ëŠ¥
            'max_bin': 256,
            'random_state': DEFAULT_SEED,
            'n_jobs': -1,
            'scale_pos_weight': scale_pos_weight,
        }
        params['max_depth'] = trial.suggest_int("max_depth", *XGB_MAX_DEPTH_RANGE)
        params['n_estimators'] = trial.suggest_int("n_estimators", *XGB_N_EST_RANGE)
        params['colsample_bytree'] = trial.suggest_categorical("colsample_bytree", [0.1, 0.3, 0.5])

        pruning_callback = XGBoostPruningCallback(trial, "valid-auc")

        num_boost_round = params.pop('n_estimators')

        booster = xgb.train(
            params,
            dtr,
            num_boost_round=num_boost_round,
            evals=[(dtr, "train"), (dva, "valid")],
            callbacks=[pruning_callback],
            verbose_eval=False,
            early_stopping_rounds=200,     # âœ… ì¶”ê°€: íŠ¸ë¼ì´ì–¼ ë‚´ ì¡°ê¸° ì¢…ë£Œ
        )

        try:
            proba = booster.predict(dva, iteration_range=(0, booster.best_iteration + 1))
        except TypeError:
            # iteration_range ë¯¸ì§€ì› ë²„ì „ ëŒ€ë¹„
            best_ntree = getattr(booster, "best_ntree_limit", None)
            if best_ntree is not None:
                proba = booster.predict(dva, ntree_limit=best_ntree)
            else:
                proba = booster.predict(dva)
        t_opt, _ = tune_cutoff(y_va, proba, strategy=strategy, train_pos_prior=train_pos_prior)
        return f1_score(y_va, (proba >= t_opt).astype(int))

    study.optimize(objective, n_trials=n_trials)

    # studyì—ì„œ ì°¾ì€ ìµœì ê°’ìœ¼ë¡œ ìµœì¢… íŒŒë¼ë¯¸í„° êµ¬ì„±
    best_params = study.best_params
    best_params.update({
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'learning_rate': 0.05,
        'subsample': 0.1,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'tree_method': 'hist',
        'max_bin': 256,
        'random_state': DEFAULT_SEED,
        'n_jobs': -1,
        'scale_pos_weight': scale_pos_weight,
    })
    return best_params


def _tune_cat_md_ne(X_tr, y_tr, X_va, y_va, cat_cols_idx, stage_key: str, strategy: str,
                    train_pos_prior: float, class_weights=None):
    n_trials = OPTUNA_TRIALS[stage_key]
    study = optuna.create_study(direction="maximize",
                                sampler=TPESampler(seed=DEFAULT_SEED),
                                pruner=MedianPruner(n_warmup_steps=5, n_min_trials=3))

    def objective(trial):
        params = {
            'loss_function': 'Logloss',
            'eval_metric': 'F1',
            'learning_rate': 0.05,
            'verbose': 0,
            'random_seed': DEFAULT_SEED,
            'depth': trial.suggest_int('depth', *CAT_DEPTH_RANGE),
        }
        if class_weights is not None:
            params["class_weights"] = class_weights

        model = CatBoostClassifier(**params, **CAT_TASK_PARAMS)
        pool_tr = Pool(X_tr, y_tr, cat_features=cat_cols_idx or None)
        pool_va = Pool(X_va, y_va, cat_features=cat_cols_idx or None)

        model.fit(pool_tr, eval_set=pool_va, use_best_model=True,
                  early_stopping_rounds=200, verbose=False)

        trial.set_user_attr("best_iteration", model.get_best_iteration())

        proba = model.predict_proba(pool_va)[:, 1]
        t_opt, _ = tune_cutoff(y_va, proba, strategy=strategy, train_pos_prior=train_pos_prior)
        return f1_score(y_va, (proba >= t_opt).astype(int))

    study.optimize(objective, n_trials=n_trials)
    best_params = study.best_params
    best_params['iterations'] = study.best_trial.user_attrs.get('best_iteration', CAT_ITER_RANGE[1])
    best_params.update({
        'loss_function': 'Logloss',
        'eval_metric': 'F1',
        'learning_rate': 0.05,
        'verbose': 0,
        'random_seed': DEFAULT_SEED,
    })
    if class_weights is not None:
        best_params["class_weights"] = class_weights
    return best_params

# ------------------------
# Optuna - Regression
# ------------------------

def _tune_lgbm_reg(X_tr, y_tr, X_va, y_va, stage_key: str, size="mid"):
    # size: "mid"(~10k; non-whale) | "tiny"(~100; whale)
    n_trials = OPTUNA_TRIALS[stage_key]
    if size == "mid":
        bounds = dict(
            n_estimators=(800, 6000),
            learning_rate=(0.01, 0.1),
            num_leaves=(63, 255),
            max_depth=(-1, 12),
            min_child_samples=(10, 200),
            subsample=(0.6, 1.0),
            colsample_bytree=(0.6, 1.0),
            reg_alpha=(0.0, 5.0),
            reg_lambda=(0.0, 10.0),
        )
    else:  # tiny
        bounds = dict(
            n_estimators=(300, 1500),
            learning_rate=(0.01, 0.2),
            num_leaves=(15, 63),
            max_depth=(3, 8),
            min_child_samples=(5, 50),
            subsample=(0.7, 1.0),
            colsample_bytree=(0.7, 1.0),
            reg_alpha=(0.0, 10.0),
            reg_lambda=(1.0, 20.0),
        )

    def objective(trial):
        params = dict(
            n_estimators=trial.suggest_int("n_estimators", *bounds["n_estimators"]),
            learning_rate=trial.suggest_float("learning_rate", *bounds["learning_rate"], log=True),
            num_leaves=trial.suggest_int("num_leaves", *bounds["num_leaves"]),
            max_depth=trial.suggest_int("max_depth", *bounds["max_depth"]),
            min_child_samples=trial.suggest_int("min_child_samples", *bounds["min_child_samples"]),
            subsample=trial.suggest_float("subsample", *bounds["subsample"]),
            colsample_bytree=trial.suggest_float("colsample_bytree", *bounds["colsample_bytree"]),
            reg_alpha=trial.suggest_float("reg_alpha", *bounds["reg_alpha"]),
            reg_lambda=trial.suggest_float("reg_lambda", *bounds["reg_lambda"]),
            objective="mae",
            random_state=SEED,
            n_jobs=max(1, (os.cpu_count() or 8)//4),
            verbosity=-1,
            force_row_wise=True,
        )
        model = lgb.LGBMRegressor(**params)
        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric="l1",
                  callbacks=[lgb.early_stopping(200, verbose=False)])
        pred = model.predict(X_va)
        return mean_absolute_error(y_va, pred)

    study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=OPTUNA_SEED))
    study.optimize(objective, n_trials=n_trials)
    best = study.best_params
    best.update(dict(objective="mae", random_state=SEED, n_jobs=max(1, (os.cpu_count() or 8)//4)))
    # ðŸ”‡ ë™ì¼ ì ìš©
    best.update({"verbosity": -1,
                 "force_row_wise": True,
    })
    return best


def _tune_cat_reg(X_tr, y_tr, X_va, y_va, cat_cols_idx, stage_key: str, size="mid"):
    n_trials = OPTUNA_TRIALS[stage_key]
    if size == "mid":
        bounds = dict(iterations=(1500, 6000), depth=(6, 10), l2=(1.0, 10.0))
    else:  # tiny
        bounds = dict(iterations=(600, 2000), depth=(4, 7), l2=(3.0, 15.0))

    def objective(trial):
        params = dict(
            depth=trial.suggest_int("depth", *bounds["depth"]),
            learning_rate=trial.suggest_float("learning_rate", 0.02, 0.2, log=True),
            l2_leaf_reg=trial.suggest_float("l2_leaf_reg", *bounds["l2"]),
            bagging_temperature=trial.suggest_float("bagging_temperature", 0.0, 3.0),
            random_strength=trial.suggest_float("random_strength", 0.0, 2.0),
            iterations=trial.suggest_int("iterations", *bounds["iterations"]),
            loss_function="MAE",
            random_seed=SEED,
            verbose=0,
        )
        model = CatBoostRegressor(**params)
        pool_tr = Pool(X_tr, y_tr, cat_features=cat_cols_idx or None)
        pool_va = Pool(X_va, y_va, cat_features=cat_cols_idx or None)
        model.fit(pool_tr, eval_set=pool_va, use_best_model=True, verbose=False)
        pred = model.predict(pool_va)
        return mean_absolute_error(y_va, pred)

    study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=OPTUNA_SEED))
    study.optimize(objective, n_trials=n_trials)
    best = study.best_params
    best.update(dict(loss_function="MAE", random_seed=SEED, verbose=0))
    return best

# ------------------------
# Stage1 â€“ Cat + LGBM + XGB (hard vote) with dual-strategy selection
# ------------------------

def train_stage1_models(
    X_tr, y_tr, X_va, y_va, cat_cols_idx, _unused_strategy, pos_prior,
    lgb_train, lgb_valid, dtr, dva,
    pool_tr=None, pool_va=None
):
    # --- CatBoost (Pool) : í•„ìš”í•  ë•Œë§Œ ìƒì„±í•˜ê³  ìž¬ì‚¬ìš©
    if pool_tr is None:
        pool_tr = Pool(X_tr, y_tr, cat_features=cat_cols_idx or None)
    if pool_va is None:
        pool_va = Pool(X_va, y_va, cat_features=cat_cols_idx or None)

    # CatBoost íŠœë‹ + í•™ìŠµ
    cat_params = _tune_cat_md_ne(
        X_tr, y_tr, X_va, y_va, cat_cols_idx,
        stage_key="stage1", strategy="prior",
        train_pos_prior=pos_prior, class_weights=None
    )
    cat1 = CatBoostClassifier(**cat_params, **CAT_TASK_PARAMS, od_type="Iter", od_wait=200)
    cat1.fit(pool_tr, eval_set=pool_va, use_best_model=True, verbose=False)
    p_cat = cat1.predict_proba(pool_va)[:, 1]

    # LightGBM
    lgb_params = _tune_lgbm_md_ne_fixedgrid(
        X_tr, y_tr, X_va, y_va, stage_key="stage1",
        strategy="prior", train_pos_prior=pos_prior,
        lgb_train=lgb_train, lgb_valid=lgb_valid
    )
    lgbm1 = lgb.LGBMClassifier(**lgb_params)
    lgbm1.fit(
        X_tr, y_tr,
        eval_set=[(X_va, y_va)],
        eval_metric="auc",
        callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)]
    )
    p_lgb = lgbm1.predict_proba(X_va)[:, 1]

    # XGBoost
    xgb_params = _tune_xgb_md_ne_fixedgrid(
        X_tr, y_tr, X_va, y_va, stage_key="stage1",
        strategy="prior", train_pos_prior=pos_prior,
        scale_pos_weight=1.0, dtr=dtr, dva=dva
    )
    xgb1 = XGBCompat(**xgb_params)
    xgb1.fit(X_tr, y_tr, X_va, y_va, early_stopping_rounds=100, verbose_eval=False)
    p_xgb = xgb1.predict_proba(X_va)[:, 1]

    # ì»·ì˜¤í”„ ì„ íƒ + í•˜ë“œë³´íŒ…
    preds = {"cat": p_cat, "lgbm": p_lgb, "xgb": p_xgb}
    cut_prior, cut_rew = {}, {}
    for k, p in preds.items():
        cut_prior[k] = tune_cutoff(y_va, p, strategy="prior",    train_pos_prior=pos_prior)[0]
        cut_rew[k]   = tune_cutoff(y_va, p, strategy="reweight", train_pos_prior=pos_prior)[0]

    yhat_prior = hard_vote(preds, cut_prior)
    yhat_rew   = hard_vote(preds, cut_rew)
    f1_prior = f1_score(y_va, yhat_prior)
    f1_rew   = f1_score(y_va, yhat_rew)

    if f1_prior >= f1_rew:
        return {"cat": cat1, "lgbm": lgbm1, "xgb": xgb1}, preds, cut_prior, "prior", f1_prior
    else:
        return {"cat": cat1, "lgbm": lgbm1, "xgb": xgb1}, preds, cut_rew,   "reweight", f1_rew


def hard_vote(preds: Dict[str, np.ndarray], cutoffs: Dict[str, float]) -> np.ndarray:
    votes = []
    for k, p in preds.items():
        t = cutoffs[k]
        votes.append((p >= t).astype(int))
    votes = np.column_stack(votes)
    return (votes.sum(axis=1) >= int(math.ceil(votes.shape[1]/2))).astype(int)

# ------------------------
# Stage2 â€“ Cat + LGBM + TabPFN (hard vote) with dual-strategy selection
# ------------------------

def train_stage2_models(X_tr, y_tr, X_va, y_va, cat_cols_idx, _unused_strategy, pos_prior):
    def _weights(y):
        n_pos = int(y.sum()); n_neg = int(len(y) - n_pos)
        w_pos = (n_neg / max(n_pos, 1)) if n_pos > 0 else 1.0
        return 1.0, w_pos

    best = {"f1": -1.0, "strategy": None}
    for strat in ["prior", "reweight"]:
        models = {}; preds = {}; cutoffs = {}
        w_neg, w_pos = _weights(y_tr)

        # CatBoost
        if USE_OPTUNA:
            cat_params = _tune_cat_cls(X_tr, y_tr, X_va, y_va, cat_cols_idx,
                                       stage_key="stage2", strategy=strat, train_pos_prior=pos_prior,
                                       class_weights=[w_neg, w_pos] if strat=="reweight" else None, size="small")
        else:
            cat_params = dict(depth=6, learning_rate=0.05, iterations=2500, loss_function="Logloss",
                              eval_metric="F1", random_seed=SEED, verbose=0)
            if strat == "reweight":
                cat_params.update(class_weights=[w_neg, w_pos])
        # CatBoost (Pool)
        cat2 = CatBoostClassifier(**cat_params, **CAT_TASK_PARAMS)
        pool_tr = Pool(X_tr, y_tr, cat_features=cat_cols_idx or None)
        pool_va = Pool(X_va, y_va, cat_features=cat_cols_idx or None)
        cat2.fit(pool_tr, eval_set=pool_va, use_best_model=True, verbose=False)
        p_cat = cat2.predict_proba(pool_va)[:, 1]
        t_cat, _ = tune_cutoff(y_va, p_cat, strategy=strat, train_pos_prior=pos_prior)
        models["cat"], preds["cat"], cutoffs["cat"] = cat2, p_cat, t_cat

        # LightGBM
        if USE_OPTUNA:
            lgb_params = _tune_lgbm_cls(X_tr, y_tr, X_va, y_va, stage_key="stage2",
                                        strategy=strat, train_pos_prior=pos_prior, size="small")
            if strat == "reweight":
                lgb_params.update(class_weight={0:w_neg, 1:w_pos})
        else:
            lgb_params = dict(n_estimators=4500, learning_rate=0.03, num_leaves=63, subsample=0.8, colsample_bytree=0.8,
                              objective="binary", random_state=SEED, n_jobs=max(1, (os.cpu_count() or 8)//4))
            if strat == "reweight":
                lgb_params.update(class_weight={0:w_neg, 1:w_pos})
        lgbm2 = lgb.LGBMClassifier(**lgb_params)
        lgbm2.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], eval_metric="auc",
                  callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(0)])
        p_lgb = lgbm2.predict_proba(X_va)[:,1]
        t_lgb, _ = tune_cutoff(y_va, p_lgb, strategy=strat, train_pos_prior=pos_prior)
        models["lgbm"], preds["lgbm"], cutoffs["lgbm"] = lgbm2, p_lgb, t_lgb

        # TabPFNì€ íŠœë‹ ì—†ìŒ(ê³ ì •)
        if _HAS_TABPFN:
            logging.info(f"[TabPFN] device set to: {DEVICE}")   # â† ì—¬ê¸°!
            tab2 = _make_tabpfn_classifier(device=DEVICE, seed=SEED, n_ens=16)
            tab2.fit(np.asarray(X_tr), np.asarray(y_tr))
            p_tab = tab2.predict_proba(np.asarray(X_va))[:, 1]
            t_tab, _ = tune_cutoff(y_va, p_tab, strategy=strat, train_pos_prior=pos_prior)
            models["tab"], preds["tab"], cutoffs["tab"] = tab2, p_tab, t_tab

        yhat = hard_vote(preds, cutoffs)
        f1_val = f1_score(y_va, yhat)
        if f1_val > best["f1"]:
            best.update(f1=f1_val, strategy=strat, models=models, preds=preds, cutoffs=cutoffs)

    return best["models"], best["preds"], best["cutoffs"], best["strategy"], best["f1"]

# ------------------------
# Stage3 â€“ TWO-HEAD regressors (whale / non-whale) with mean & median ensembles
# ------------------------

def train_stage3_regressors_twohead(X_pay_tr, y_amt_tr, y_whale_tr,
                                    X_pay_va, y_amt_va, y_whale_va, cat_cols_idx):
    """
    Stage3 TWO-HEAD íšŒê·€ (non-whale / whale)
    - ê° í—¤ë“œ: CatBoostRegressor + LGBMRegressor + (ì˜µì…˜) TabPFNRegressor
    - Optunaë¡œ Cat/LGBM íŠœë‹(ì´ë¯¸ ìƒë‹¨ í—¬í¼ ì‚¬ìš©), TabPFNì€ ê³ ì • íŒŒë¼ë¯¸í„°
    - VALì—ì„œ ë‘ í—¤ë“œ ê°ê° ì˜ˆì¸¡ í›„ payerë³„ë¡œ ë¶™ì—¬ mean/median ì•™ìƒë¸” ë°˜í™˜
    """
    tr0 = np.where(y_whale_tr == 0)[0]; tr1 = np.where(y_whale_tr == 1)[0]
    va0 = np.where(y_whale_va == 0)[0]; va1 = np.where(y_whale_va == 1)[0]

    # ---------- non-whale í—¤ë“œ (~1e4) ----------
    if USE_OPTUNA:
        cat_params0 = _tune_cat_reg(X_pay_tr.iloc[tr0], y_amt_tr[tr0],
                                    X_pay_va.iloc[va0], y_amt_va[va0],
                                    cat_cols_idx, stage_key="stage3_nw", size="mid")
        lgb_params0 = _tune_lgbm_reg(X_pay_tr.iloc[tr0], y_amt_tr[tr0],
                                     X_pay_va.iloc[va0], y_amt_va[va0],
                                     stage_key="stage3_nw", size="mid")
    else:
        cat_params0 = dict(depth=8, learning_rate=0.05, iterations=5000,
                           loss_function="MAE", random_seed=SEED, verbose=0)
        lgb_params0 = dict(n_estimators=6000, learning_rate=0.03, num_leaves=127,
                           subsample=0.8, colsample_bytree=0.8, objective="mae",
                           random_state=SEED, n_jobs=max(1, (os.cpu_count() or 8)//4))

    catR0 = CatBoostRegressor(**cat_params0, **CAT_TASK_PARAMS)
    lgbR0 = lgb.LGBMRegressor(**lgb_params0)

    catR0.fit(
        Pool(X_pay_tr.iloc[tr0], y_amt_tr[tr0], cat_features=cat_cols_idx or None),
        eval_set=Pool(X_pay_va.iloc[va0], y_amt_va[va0], cat_features=cat_cols_idx or None),
        use_best_model=True, verbose=False
    )
    lgbR0.fit(X_pay_tr.iloc[tr0], y_amt_tr[tr0],
              eval_set=[(X_pay_va.iloc[va0], y_amt_va[va0])], eval_metric="l1",
              callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(0)])

    # + TabPFN (ì˜µì…˜)
    tabR0 = None
    if _HAS_TABPFN:
        logging.info(f"[TabPFN] device set to: {DEVICE}")   # â† ì—¬ê¸°!
        tabR0 = _make_tabpfn_regressor(device=DEVICE, seed=SEED, n_ens=16)
        tabR0.fit(np.asarray(X_pay_tr.iloc[tr0]), np.asarray(y_amt_tr[tr0]))

    # ---------- whale í—¤ë“œ (~1e2) ----------
    if USE_OPTUNA:
        cat_params1 = _tune_cat_reg(X_pay_tr.iloc[tr1], y_amt_tr[tr1],
                                    X_pay_va.iloc[va1], y_amt_va[va1],
                                    cat_cols_idx, stage_key="stage3_w", size="tiny")
        lgb_params1 = _tune_lgbm_reg(X_pay_tr.iloc[tr1], y_amt_tr[tr1],
                                     X_pay_va.iloc[va1], y_amt_va[va1],
                                     stage_key="stage3_w", size="tiny")
    else:
        cat_params1 = dict(depth=6, learning_rate=0.05, iterations=1500,
                           loss_function="MAE", random_seed=SEED, verbose=0, l2_leaf_reg=8.0)
        lgb_params1 = dict(n_estimators=1200, learning_rate=0.05, num_leaves=31,
                           subsample=0.9, colsample_bytree=0.9, objective="mae",
                           random_state=SEED, n_jobs=max(1, (os.cpu_count() or 8)//4), reg_lambda=10.0)

    catR1 = CatBoostRegressor(**cat_params1, **CAT_TASK_PARAMS)
    lgbR1 = lgb.LGBMRegressor(**lgb_params1)

    catR1.fit(
        Pool(X_pay_tr.iloc[tr1], y_amt_tr[tr1], cat_features=cat_cols_idx or None),
        eval_set=Pool(X_pay_va.iloc[va1], y_amt_va[va1], cat_features=cat_cols_idx or None),
        use_best_model=True, verbose=False
    )
    lgbR1.fit(X_pay_tr.iloc[tr1], y_amt_tr[tr1],
              eval_set=[(X_pay_va.iloc[va1], y_amt_va[va1])], eval_metric="l1",
              callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(0)])

    # + TabPFN (ì˜µì…˜)
    tabR1 = None
    if _HAS_TABPFN:
        logging.info(f"[TabPFN] device set to: {DEVICE}")   # â† ì—¬ê¸°!
        tabR1 = _make_tabpfn_regressor(device=DEVICE, seed=SEED, n_ens=16)
        tabR1.fit(np.asarray(X_pay_tr.iloc[tr1]), np.asarray(y_amt_tr[tr1]))

    # ---------- VAL ì˜ˆì¸¡(ë‘ í—¤ë“œ) ----------
    def _pred(models_head, X_local):
        parts = [
            models_head["cat"].predict(Pool(X_local, cat_features=cat_cols_idx or None)),
            models_head["lgbm"].predict(X_local),
        ]
        if "tab" in models_head:
            parts.append(models_head["tab"].predict(np.asarray(X_local)))
        P = np.column_stack(parts)
        return np.mean(P, axis=1), np.median(P, axis=1)

    models = {
        "nonwhale": {"cat": catR0, "lgbm": lgbR0},
        "whale":    {"cat": catR1, "lgbm": lgbR1},
    }
    if _HAS_TABPFN:
        models["nonwhale"]["tab"] = tabR0
        models["whale"]["tab"]    = tabR1

    n_va = len(X_pay_va)
    va_mean = np.zeros(n_va, dtype=float)
    va_med  = np.zeros(n_va, dtype=float)

    if len(va0):
        m0, md0 = _pred(models["nonwhale"], X_pay_va.iloc[va0])
        va_mean[va0] = m0; va_med[va0] = md0
    if len(va1):
        m1, md1 = _pred(models["whale"], X_pay_va.iloc[va1])
        va_mean[va1] = m1; va_med[va1] = md1

    return models, {"mean": va_mean, "median": va_med}

# ------------------------
# End-to-end
# ------------------------

def run_pipeline(seed: int = 2025):
    global SEED, OPTUNA_SEED
    SEED = seed
    OPTUNA_SEED = seed
    np.random.seed(SEED)
    random.seed(SEED)

    # Load
    train, val, test = load_pre_split()

    # Stage1 labels
    y_tr = (train[TARGET_COL] > 0).astype(int)
    y_va = (val[TARGET_COL] > 0).astype(int)

    # Features
    drop_cols = [ID_COL, TARGET_COL]
    Xtr_raw, feat_cols, cat_cols = build_features(train, TARGET_COL, drop_cols)
    Xva_raw = val[feat_cols].copy(); Xte_raw = test[feat_cols].copy()

    enc = OrdinalCategoryEncoder().fit(Xtr_raw, cat_cols)
    Xtr = enc.transform(Xtr_raw); Xva = enc.transform(Xva_raw); Xte = enc.transform(Xte_raw)
    cat_cols_idx = [Xtr.columns.get_loc(c) for c in cat_cols if c in Xtr.columns]

    num_cols, med = fit_imputer(Xtr)
    Xtr = apply_imputer(Xtr, num_cols, med)
    Xva = apply_imputer(Xva, num_cols, med)
    Xte = apply_imputer(Xte, num_cols, med)

    # â†“ ì»¬ëŸ¼ëª… ê³µë°± ì œê±° (ê²½ê³  ì–µì œìš©)
    Xtr = _sanitize_cols(Xtr)
    Xva = _sanitize_cols(Xva)
    Xte = _sanitize_cols(Xte)

    lgb_train = lgb.Dataset(Xtr, label=y_tr, free_raw_data=False)
    lgb_valid = lgb.Dataset(Xva, label=y_va, reference=lgb_train, free_raw_data=False)

    dtr = xgb.DMatrix(Xtr, label=y_tr)
    dva = xgb.DMatrix(Xva, label=y_va)

    # Stage1 â€“ dual strategy auto-select
    pos_prior1 = float(y_tr.mean())
    m1, p1, t1, strat1, f1_best1 = train_stage1_models(
        Xtr, y_tr, Xva, y_va, cat_cols_idx, None, pos_prior1,
        lgb_train=lgb_train, lgb_valid=lgb_valid, dtr=dtr, dva=dva
    )
    yhat1_va = hard_vote(p1, t1)
    f1_1 = f1_score(y_va, yhat1_va); pr_1 = precision_score(y_va, yhat1_va); rc_1 = recall_score(y_va, yhat1_va)
    logging.info(f"[Seed {SEED}] [Stage1|VAL] F1={f1_1:.4f} Prec={pr_1:.4f} Rec={rc_1:.4f} PosPrior={pos_prior1:.4f} | ChosenStrategy={strat1} (VoteF1={f1_best1:.4f})")

    # Stage2 (payers only) â€“ dual strategy auto-select
    tr_pay_idx = np.where(y_tr == 1)[0]; va_pay_idx = np.where(y_va == 1)[0]
    y_tr_pay_amt = train.loc[train[TARGET_COL] > 0, TARGET_COL].values
    whale_cut = float(np.quantile(y_tr_pay_amt, WHALE_Q))

    y2_tr = (train.loc[train[TARGET_COL] > 0, TARGET_COL].values >= whale_cut).astype(int)
    y2_va = (val.loc[val[TARGET_COL] > 0, TARGET_COL].values >= whale_cut).astype(int)

    X2_tr = Xtr.iloc[tr_pay_idx].reset_index(drop=True)
    X2_va = Xva.iloc[va_pay_idx].reset_index(drop=True)

    pos_prior2 = float(y2_tr.mean())
    m2, p2, t2, strat2, f1_best2 = train_stage2_models(X2_tr, y2_tr, X2_va, y2_va, cat_cols_idx, None, pos_prior2)
    yhat2_va = hard_vote(p2, t2)
    f1_2 = f1_score(y2_va, yhat2_va); pr_2 = precision_score(y2_va, yhat2_va); rc_2 = recall_score(y2_va, yhat2_va)
    logging.info(f"[Seed {SEED}] [Stage2|VAL] F1={f1_2:.4f} Prec={pr_2:.4f} Rec={rc_2:.4f} PosPrior={pos_prior2:.4f} WhaleCut={whale_cut:.1f} | ChosenStrategy={strat2} (VoteF1={f1_best2:.4f})")

    # Stage3 (two-head regression on payers)
    y3_tr = train.loc[train[TARGET_COL] > 0, TARGET_COL].values.astype(float)
    y3_va = val.loc[val[TARGET_COL] > 0, TARGET_COL].values.astype(float)

    m3, p3_va = train_stage3_regressors_twohead(X2_tr, y3_tr, y2_tr, X2_va, y3_va, y2_va, cat_cols_idx)

    # ---- Calibration constants from VAL for actual ensembles ----
    Ybar = float(np.mean(y3_va))
    Xbar_mean = float(np.mean(p3_va["mean"])) if np.mean(p3_va["mean"]) != 0 else 1e-9
    Xbar_median = float(np.mean(p3_va["median"])) if np.mean(p3_va["median"]) != 0 else 1e-9
    c_mean = Ybar / Xbar_mean
    c_median = Ybar / Xbar_median
    var_mean = float(np.sum((y3_va - p3_va["mean"])**2))
    var_median = float(np.sum((y3_va - p3_va["median"])**2))

    mae_va_mean = mean_absolute_error(y3_va, p3_va["mean"]);  smape_va_mean = smape(y3_va, p3_va["mean"])
    mae_va_med  = mean_absolute_error(y3_va, p3_va["median"]); smape_va_med  = smape(y3_va, p3_va["median"])
    logging.info(f"[Seed {SEED}] [Stage3|VAL|Mean]   MAE={mae_va_mean:.2f} SMAPE={smape_va_mean:.2f}")
    logging.info(f"[Seed {SEED}] [Stage3|VAL|Median] MAE={mae_va_med:.2f}  SMAPE={smape_va_med:.2f}")

    # ---------- TEST INFERENCE ----------
    # Stage1
    pool_te = Pool(Xte, cat_features=cat_cols_idx or None)
    p1_te = {
        "cat":  m1["cat"].predict_proba(pool_te)[:, 1],
        "lgbm": m1["lgbm"].predict_proba(Xte)[:, 1],
        "xgb":  m1["xgb"].predict_proba(Xte)[:, 1],
    }
    yhat1_te = hard_vote(p1_te, t1)

    # Stage2 within predicted payers
    te_pay_idx = np.where(yhat1_te == 1)[0]
    X2_te = Xte.iloc[te_pay_idx].reset_index(drop=True)
    pool_te_pay = Pool(X2_te, cat_features=cat_cols_idx or None)
    p2_te = {
        "cat":  m2["cat"].predict_proba(pool_te_pay)[:, 1],
        "lgbm": m2["lgbm"].predict_proba(X2_te)[:, 1],
    }
    if "tab" in m2:
        p2_te["tab"] = m2["tab"].predict_proba(np.asarray(X2_te))[:, 1]
    yhat2_te = hard_vote(p2_te, t2)  # 1=whale among TEST payers

    # Stage3 routing
    te0_local = np.where(yhat2_te == 0)[0]; te1_local = np.where(yhat2_te == 1)[0]

    def _pred_head(models, X_local):
        parts = [
            models["cat"].predict(Pool(X_local, cat_features=cat_cols_idx or None)),
            models["lgbm"].predict(X_local),
        ]
        if "tab" in models:
            parts.append(models["tab"].predict(np.asarray(X_local)))
        P = np.column_stack(parts)
        return np.mean(P, axis=1), np.median(P, axis=1)

    mean0, med0 = _pred_head(m3["nonwhale"], X2_te.iloc[te0_local]) if len(te0_local)>0 else (np.array([]), np.array([]))
    mean1, med1 = _pred_head(m3["whale"],     X2_te.iloc[te1_local]) if len(te1_local)>0 else (np.array([]), np.array([]))

    final_mean = np.zeros(len(Xte), dtype=float)
    final_median = np.zeros(len(Xte), dtype=float)
    if len(te0_local)>0:
        final_mean[te_pay_idx[te0_local]] = mean0
        final_median[te_pay_idx[te0_local]] = med0
    if len(te1_local)>0:
        final_mean[te_pay_idx[te1_local]] = mean1
        final_median[te_pay_idx[te1_local]] = med1

    whale_mask_te = np.zeros(len(Xte), dtype=int)
    whale_mask_te[te_pay_idx] = yhat2_te

    out_mean = pd.DataFrame({ID_COL: test[ID_COL].values, "pred_pay_amt_sum": final_mean, "pred_is_payer": yhat1_te, "pred_is_whale": whale_mask_te})
    out_median = pd.DataFrame({ID_COL: test[ID_COL].values, "pred_pay_amt_sum": final_median, "pred_is_payer": yhat1_te, "pred_is_whale": whale_mask_te})

    # --- Calibration-based TEST predictions (actual ensemble) ---
    out_mean_calib = out_mean.copy();   out_mean_calib["pred_pay_amt_sum"] *= c_mean
    out_median_calib = out_median.copy(); out_median_calib["pred_pay_amt_sum"] *= c_median

    return {
        "val_metrics": {
            "stage1": {"f1": f1_1, "precision": pr_1, "recall": rc_1, "pos_prior": pos_prior1, "chosen_strategy": strat1},
            "stage2": {"f1": f1_2, "precision": pr_2, "recall": rc_2, "pos_prior": pos_prior2, "whale_cut": whale_cut, "chosen_strategy": strat2},
            "stage3": {
                "mean":   {"mae": mae_va_mean, "smape": smape_va_mean},
                "median": {"mae": mae_va_med,  "smape": smape_va_med },
            },
        },
        "cutoffs": {"stage1": t1, "stage2": t2},
        "chosen_strategies": {"stage1": strat1, "stage2": strat2},
        "whale_cut": whale_cut,
        "pred_test_mean": out_mean,
        "pred_test_median": out_median,
        "pred_test_mean_calibrated": out_mean_calib,
        "pred_test_median_calibrated": out_median_calib,
        "calibration_stage3": {"mean": {"c": c_mean, "variation": var_mean}, "median": {"c": c_median, "variation": var_median}},
    }

def run_all_seeds(seeds=SEEDS):
    all_results = []
    for sd in seeds:
        logging.info(f"================ Seed {sd} ================")
        res = run_pipeline(seed=sd)
        all_results.append(res)
    # Aggregate TEST predictions across seeds (element-wise)
    preds_mean = [r["pred_test_mean"]["pred_pay_amt_sum"].values for r in all_results]
    preds_median = [r["pred_test_median"]["pred_pay_amt_sum"].values for r in all_results]
    payers = [r["pred_test_mean"]["pred_is_payer"].values for r in all_results]
    whales = [r["pred_test_mean"]["pred_is_whale"].values for r in all_results]

    preds_mean = np.column_stack(preds_mean)
    preds_median = np.column_stack(preds_median)
    payers = np.column_stack(payers)
    whales = np.column_stack(whales)

    final_pred_mean = preds_mean.mean(axis=1)
    final_pred_median = np.median(preds_median, axis=1)

    # Majority vote across seeds for masks
    final_is_payer = (payers.sum(axis=1) >= int(math.ceil(payers.shape[1]/2))).astype(int)
    final_is_whale = (whales.sum(axis=1) >= int(math.ceil(whales.shape[1]/2))).astype(int)

    # Build final DataFrames using the first seed's IDs (identical order)
    any_df = all_results[0]["pred_test_mean"]
    df_mean = any_df[["PLAYERID"]].copy()
    df_mean["pred_pay_amt_sum"] = final_pred_mean
    df_mean["pred_is_payer"] = final_is_payer
    df_mean["pred_is_whale"] = final_is_whale

    df_median = any_df[["PLAYERID"]].copy()
    df_median["pred_pay_amt_sum"] = final_pred_median
    df_median["pred_is_payer"] = final_is_payer
    df_median["pred_is_whale"] = final_is_whale

    return {"by_seed": all_results, "final_mean": df_mean, "final_median": df_median, "seeds": list(seeds)}

# ================= Evaluations & Reports =================

def _metrics_basic(y_true, pred):
    mse = mean_squared_error(y_true, pred)
    mae = mean_absolute_error(y_true, pred)
    rmse = float(np.sqrt(mse))
    sm = smape(y_true, pred)
    return mse, mae, rmse, sm

def _fmt2(x):
    if x is None or (isinstance(x, float) and (pd.isna(x) or pd.isnull(x))):
        return "NA"
    try:
        return f"{float(x):,.2f}"
    except Exception:
        return str(x)

# === table helpers (SE í¬í•¨, í‘œì‹œ ê¹”ë”í™”) ===
def make_top_groups_table(y, p, P_seeds=None, top_qs=(0.99, 0.97, 0.95, 0.90, 0.80)):
    y = np.asarray(y).reshape(-1)
    p = np.asarray(p).reshape(-1)
    pos_mask = (y > 0)
    if not pos_mask.any():
        return pd.DataFrame(columns=["ìƒìœ„ í¼ì„¼íŠ¸","ìƒ˜í”Œ ìˆ˜","MAE","SMAPE","ì˜ˆì¸¡í‰ê· (SE)","ì‹¤ì œ í‰ê· "])

    y_pos = y[pos_mask]
    rows = []
    S = None if P_seeds is None else P_seeds.shape[1]
    for q in top_qs:
        thr = np.quantile(y_pos, q)
        idx = np.where((y > 0) & (y >= thr))[0]
        if idx.size == 0:
            continue

        mae = mean_absolute_error(y[idx], p[idx])
        sm  = smape(y[idx], p[idx])
        pred_mean = p[idx].mean()

        if P_seeds is not None:
            m_per_seed = P_seeds[idx].mean(axis=0)   # (S,)
            se = (m_per_seed.std(ddof=1) / np.sqrt(S)) if S and S > 1 else 0.0
            se_str = _fmt2(se)
        else:
            se_str = "NA"

        rows.append({
            "ìƒìœ„ í¼ì„¼íŠ¸": f"{int((1-q)*100)}%",
            "ìƒ˜í”Œ ìˆ˜": f"{int(idx.size):,}",
            "MAE": _fmt2(mae),
            "SMAPE": _fmt2(sm),
            "ì˜ˆì¸¡í‰ê· (SE)": f"{_fmt2(pred_mean)} ({se_str})",
            "ì‹¤ì œ í‰ê· ": _fmt2(y[idx].mean()),
        })

    return pd.DataFrame(rows, columns=["ìƒìœ„ í¼ì„¼íŠ¸","ìƒ˜í”Œ ìˆ˜","MAE","SMAPE","ì˜ˆì¸¡í‰ê· (SE)","ì‹¤ì œ í‰ê· "])


def make_quantile_bins_table(y, p, P_seeds=None, bins=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)):
    y = np.asarray(y).reshape(-1)
    p = np.asarray(p).reshape(-1)
    pos_mask = (y > 0)
    if not pos_mask.any():
        return pd.DataFrame(columns=["Quantile êµ¬ê°„","ìƒ˜í”Œ ìˆ˜","MAE","SMAPE","ì˜ˆì¸¡í‰ê· (SE)","ì‹¤ì œ í‰ê· "])

    y_pos = y[pos_mask]
    qs = np.quantile(y_pos, bins)
    rows = []
    S = None if P_seeds is None else P_seeds.shape[1]
    for i in range(len(qs)-1):
        lo, hi = qs[i], qs[i+1]
        if i < len(qs)-2:
            idx = np.where((y > 0) & (y >= lo) & (y <  hi))[0]
        else:
            idx = np.where((y > 0) & (y >= lo) & (y <= hi))[0]
        if idx.size == 0:
            continue

        mae = mean_absolute_error(y[idx], p[idx])
        sm  = smape(y[idx], p[idx])
        pred_mean = p[idx].mean()

        if P_seeds is not None:
            m_per_seed = P_seeds[idx].mean(axis=0)   # (S,)
            se = (m_per_seed.std(ddof=1) / np.sqrt(S)) if S and S > 1 else 0.0
            se_str = _fmt2(se)
        else:
            se_str = "NA"

        rows.append({
            "Quantile êµ¬ê°„": f"{int(bins[i]*100)}% ~ {int(bins[i+1]*100)}%",
            "ìƒ˜í”Œ ìˆ˜": f"{int(idx.size):,}",
            "MAE": _fmt2(mae),
            "SMAPE": _fmt2(sm),
            "ì˜ˆì¸¡í‰ê· (SE)": f"{_fmt2(pred_mean)} ({se_str})",
            "ì‹¤ì œ í‰ê· ": _fmt2(y[idx].mean()),
        })

    return pd.DataFrame(rows, columns=["Quantile êµ¬ê°„","ìƒ˜í”Œ ìˆ˜","MAE","SMAPE","ì˜ˆì¸¡í‰ê· (SE)","ì‹¤ì œ í‰ê· "])

# === í‰ê°€ 1: ì‹œë“œë³„ + ì•™ìƒë¸”(ë³´ì • ì „) ======================================
def evaluate_after_seeds(agg_result: Dict):
    from IPython.display import display

    # í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ
    try:
        test_df = pd.read_parquet(DATA_PATHS["test"])
    except Exception:
        logging.info("[WARN] Cannot load test parquet for evaluation.")
        return
    if TARGET_COL not in test_df.columns:
        logging.info("[INFO] Test set has no target column. Skipping evaluation.")
        return

    y = test_df[TARGET_COL].values.reshape(-1)

    # ì‹œë“œë³„ ì˜ˆì¸¡(Mean path)
    seeds = agg_result.get("seeds", SEEDS)
    P = np.column_stack([
        r["pred_test_mean"]["pred_pay_amt_sum"].values
        for r in agg_result["by_seed"]
    ])  # (N, S)

    # --- Per-seed report ---
    logging.info("="*80)
    logging.info("ðŸ”¶ Per-Seed Results (ê° ì‹œë“œì˜ ë‹¨ì¼ ëª¨ë¸ ì˜ˆì¸¡)")
    for sd, col in zip(seeds, P.T):
        mse, mae, rmse, sm = _metrics_basic(y, col)
        logging.info(f"[Seed {sd}] MAE: {mae:,.2f} | RMSE: {rmse:,.2f} | SMAPE: {sm:.2f}%")
        df_top  = make_top_groups_table(y, col, P_seeds=None)
        df_bins = make_quantile_bins_table(y, col, P_seeds=None)
        logging.info("  Â· ì§€ì¶œ ìƒìœ„ ê·¸ë£¹")
        logging.info(f"\n{df_top.to_string()}")
        logging.info("  Â· Quantile êµ¬ê°„")
        logging.info(f"\n{df_bins.to_string()}")

    # --- Ensemble mean (ë³´ì • ì „) ---
    ens_mean = P.mean(axis=1)
    mse, mae, rmse, sm = _metrics_basic(y, ens_mean)
    logging.info("="*80)
    logging.info("ðŸ”· FINAL RESULT â€” Seed Ensemble (MEAN, uncalibrated)")
    logging.info(f"MAE: {mae:,.2f} | RMSE: {rmse:,.2f} | SMAPE: {sm:.2f}%")

    df_top_ens  = make_top_groups_table(y, ens_mean, P_seeds=P)
    df_bins_ens = make_quantile_bins_table(y, ens_mean, P_seeds=P)
    logging.info("\nðŸ“ˆ  [ì§€ì¶œ ìƒìœ„ ê·¸ë£¹ ê¸°ì¤€] í‰ê·  / SE / MAE / SMAPE")
    logging.info(df_top_ens)
    logging.info("\nðŸ“Š  [Quantile êµ¬ê°„ë³„] í‰ê·  / SE / MAE / SMAPE")
    logging.info(df_bins_ens)

    return {
        "per_seed": {"metrics": [
            {"seed": int(sd), "MAE": float(_metrics_basic(y, col)[1]),
             "RMSE": float(_metrics_basic(y, col)[2]),
             "SMAPE": float(_metrics_basic(y, col)[3])}
            for sd, col in zip(seeds, P.T)
        ]},
        "ensemble_mean": {
            "metrics": {"MAE": mae, "RMSE": rmse, "SMAPE": sm},
            "df_top": df_top_ens, "df_bins": df_bins_ens
        }
    }

def run_stage1_only(trials=50, seed=2025, is_test_mode=False):
    global SEED, OPTUNA_SEED
    SEED = int(seed)
    OPTUNA_SEED = int(seed)          # Optunaë„ ì‹œë“œë³„ë¡œ ìž¬í˜„ë˜ê²Œ
    np.random.seed(SEED)
    random.seed(SEED)

    OPTUNA_TRIALS["stage1"] = int(trials)

    train, val, _ = load_pre_split(is_test_mode=is_test_mode)
    y_tr = (train[TARGET_COL] > 0).astype(int)
    y_va = (val[TARGET_COL] > 0).astype(int)

    drop_cols = [ID_COL, TARGET_COL]
    Xtr_raw, feat_cols, cat_cols = build_features(train, TARGET_COL, drop_cols)
    Xva_raw = val[feat_cols].copy()

    enc = OrdinalCategoryEncoder().fit(Xtr_raw, cat_cols)
    Xtr = enc.transform(Xtr_raw); Xva = enc.transform(Xva_raw)
    cat_cols_idx = [Xtr.columns.get_loc(c) for c in cat_cols if c in Xtr.columns]

    num_cols, med = fit_imputer(Xtr)
    Xtr = apply_imputer(Xtr, num_cols, med); Xva = apply_imputer(Xva, num_cols, med)
    Xtr = _sanitize_cols(Xtr); Xva = _sanitize_cols(Xva)

    # === ìž¬ì‚¬ìš© ì»¨í…Œì´ë„ˆ(í•œ ë²ˆë§Œ ìƒì„±) ===
    lgb_train = lgb.Dataset(Xtr, label=y_tr, free_raw_data=False)
    lgb_valid = lgb.Dataset(Xva, label=y_va, reference=lgb_train, free_raw_data=False)

    dtr = xgb.DMatrix(Xtr, label=y_tr)
    dva = xgb.DMatrix(Xva, label=y_va)

    pos_prior1 = float(y_tr.mean())
    pool_tr = Pool(Xtr, y_tr, cat_features=cat_cols_idx or None)
    pool_va = Pool(Xva, y_va, cat_features=cat_cols_idx or None)

    models, preds, cutoffs, chosen_strategy, vote_f1 = train_stage1_models(
        Xtr, y_tr, Xva, y_va, cat_cols_idx, None, pos_prior1,
        lgb_train=lgb_train, lgb_valid=lgb_valid, dtr=dtr, dva=dva,
        pool_tr=pool_tr, pool_va=pool_va
    )
    yhat_va = hard_vote(preds, cutoffs)
    f1  = f1_score(y_va, yhat_va)
    prc = precision_score(y_va, yhat_va, zero_division=0)
    rcl = recall_score(y_va, yhat_va, zero_division=0)

    # ðŸ”¹ AUC(í™•ë¥  í‰ê· )
    try:
        prob_ens = (preds["cat"] + preds["lgbm"] + preds["xgb"]) / 3.0
        auc = roc_auc_score(y_va, prob_ens)
    except Exception:
        auc = float("nan")

    # ðŸ”¹ Best params ì¶”ì¶œ(í•µì‹¬ íŒŒë¼ë¯¸í„°ë§Œ)
    best_lgb_all = models["lgbm"].get_params()
    best_xgb_all = models["xgb"].params
    best_cat_all = models["cat"].get_params()

    best_lgb = _select_params("lgbm", best_lgb_all)
    best_xgb = _select_params("xgb",  best_xgb_all)
    best_cat = _select_params("cat",  best_cat_all)

    logging.info(f"[Seed {SEED}] [Stage1|VAL] "
                 f"F1={f1:.4f} | Precision={prc:.4f} | Recall={rcl:.4f} | AUC={auc:.4f} "
                 f"| PosPrior={pos_prior1:.4f} | Strategy={chosen_strategy}")
    logging.info(f"[Best LGBM]: {best_lgb}")
    logging.info(f"[Best XGB ]: {best_xgb}")
    logging.info(f"[Best CAT ]: {best_cat}")

    return {
        "seed": SEED,
        "Xtr":Xtr, "y_tr":np.asarray(y_tr), "Xva":Xva, "y_va":np.asarray(y_va),
        "models":models, "cutoffs":cutoffs, "preds":preds, "strategy":chosen_strategy,
        "F1":f1, "Precision":prc, "Recall":rcl, "AUC":auc, "cat_cols_idx": cat_cols_idx,
        "best_params": {"lgbm": best_lgb, "xgb": best_xgb, "cat": best_cat}
    }

def plot_lgbm_error_trajectory(Xtr, y_tr, Xva, y_va, best_lgb_params: dict, n_estimators_big: int = 6000):
    params = LGBM_FIXED.copy()
    params.update(best_lgb_params)
    params["n_estimators"] = int(n_estimators_big)  # ê¸¸ê²Œ ì¤˜ì„œ ê¶¤ì  í™•ì¸

    model = lgb.LGBMClassifier(**params)
    model.fit(
        Xtr, y_tr,
        eval_set=[(Xva, y_va)],
        eval_metric="auc",
        callbacks=[lgb.log_evaluation(0)],   # ì „ êµ¬ê°„ ê¸°ë¡, ì¡°ê¸°ì¢…ë£Œ X
    )
    ev = model.evals_result_
    key = "valid_0" if "valid_0" in ev else "validation_0"
    auc_list = ev[key]["auc"]
    iters = np.arange(1, len(auc_list)+1)
    val_error = 1.0 - np.array(auc_list, dtype=float)

    plt.figure(figsize=(7,4.2))
    plt.plot(iters, val_error, linewidth=1.5)
    md = params.get("max_depth", "NA"); cbt = params.get("colsample_bytree", "NA")
    plt.title(f"LGBM Error Trajectory | max_depth={md}, colsample={cbt}, lr={params.get('learning_rate')}")
    plt.xlabel("n_estimators (boosting rounds)")
    plt.ylabel("Validation Error (1 - AUC)")
    plt.grid(True, linewidth=0.3)
    plt.savefig(RESULTS_DIR / f"lgbm_error_trajectory_{SEED}.png") # âœ… íŒŒì¼ë¡œ ì €ìž¥í•˜ëŠ” ì½”ë“œ ì¶”ê°€
    plt.close() # âœ… ë©”ëª¨ë¦¬ í•´ì œë¥¼ ìœ„í•´ ì¶”ê°€

    best_idx = int(np.argmin(val_error))
    logging.info(f"ðŸ”Ž ìµœì†Œ ì—ëŸ¬ ì§€ì : iter={best_idx+1:,} | 1-AUC={val_error[best_idx]:.6f} | AUC={1-val_error[best_idx]:.6f}")

def plot_xgb_error_trajectory(Xtr, y_tr, Xva, y_va, best_xgb_params: dict, n_estimators_big: int = 6000):
    """
    best_xgb_params: stage1 íŠœë‹ ê²°ê³¼(models['xgb'].params)ì—ì„œ êº¼ë‚¸ dict ì‚¬ìš© ê¶Œìž¥
      - ì‚¬ìš©ë˜ëŠ” í‚¤: max_depth, colsample_bytree, subsample, learning_rate, reg_alpha, reg_lambda, scale_pos_weight(ì˜µì…˜)
    n_estimators_big: ê¶¤ì ì„ ë³¼ ë§Œí¼ í¬ê²Œ ì¤Œ (ì¡°ê¸°ì¢…ë£Œ ì—†ìŒ)
    """
    # í•™ìŠµ/ê²€ì¦ DMatrix
    dtr = xgb.DMatrix(Xtr, label=y_tr)
    dva = xgb.DMatrix(Xva, label=y_va)

    # íŒŒë¼ë¯¸í„° êµ¬ì„± (AUC ê¸°ì¤€ìœ¼ë¡œ ì—ëŸ¬ê³¡ì„  = 1 - AUC)
    params = {
        "objective": "binary:logistic",
        "eval_metric": "auc",
        "tree_method": "hist",
        "max_bin": 256,
        "random_state": best_xgb_params.get("random_state", 2025),
        "nthread": best_xgb_params.get("n_jobs", max(1, (os.cpu_count() or 8)//4)),
        "learning_rate": best_xgb_params.get("learning_rate", 0.05),
        "max_depth": best_xgb_params.get("max_depth", 8),
        "subsample": best_xgb_params.get("subsample", 0.1),
        "colsample_bytree": best_xgb_params.get("colsample_bytree", 0.3),
        "reg_alpha": best_xgb_params.get("reg_alpha", 0.1),
        "reg_lambda": best_xgb_params.get("reg_lambda", 0.1),
    }
    if "scale_pos_weight" in best_xgb_params:
        params["scale_pos_weight"] = best_xgb_params["scale_pos_weight"]

    evals_result = {}
    booster = xgb.train(
        params,
        dtr,
        num_boost_round=int(n_estimators_big),
        evals=[(dtr, "train"), (dva, "valid")],
        evals_result=evals_result,
        verbose_eval=False,     # ì „ì²´ ê¶¤ì ì„ ì¡°ìš©ížˆ ìˆ˜ì§‘
    )

    auc_list = evals_result["valid"]["auc"]
    iters = np.arange(1, len(auc_list) + 1)
    val_error = 1.0 - np.array(auc_list, dtype=float)

    plt.figure(figsize=(7, 4.2))
    plt.plot(iters, val_error, linewidth=1.5)
    plt.title(f"XGBoost Error Trajectory | max_depth={params['max_depth']}, colsample={params['colsample_bytree']}, lr={params['learning_rate']}")
    plt.xlabel("n_estimators (boosting rounds)")
    plt.ylabel("Validation Error (1 - AUC)")
    plt.grid(True, linewidth=0.3)
    plt.savefig(RESULTS_DIR / f"xgb_error_trajectory_{SEED}.png") # íŒŒì¼ ì´ë¦„ì€ í•¨ìˆ˜ì— ë§žê²Œ ë³€ê²½
    plt.close()


    best_idx = int(np.argmin(val_error))
    logging.info(f"ðŸ”Ž [XGB] ìµœì†Œ ì—ëŸ¬ ì§€ì : iter={best_idx+1:,} | 1-AUC={val_error[best_idx]:.6f} | AUC={1 - val_error[best_idx]:.6f}")

def plot_cat_error_trajectory(Xtr, y_tr, Xva, y_va, cat_cols_idx, best_cat_params: dict, n_estimators_big: int = 6000):
    """
    best_cat_params: stage1 íŠœë‹ ê²°ê³¼(models['cat'].get_params())ì—ì„œ êº¼ë‚¸ dict ì‚¬ìš© ê¶Œìž¥
      - ì‚¬ìš©ë˜ëŠ” í‚¤: depth, learning_rate(ê³ ì • 0.05), class_weights(ìžˆì„ ìˆ˜ ìžˆìŒ)
    n_estimators_big: ê¶¤ì ì„ ë³¼ ë§Œí¼ í¬ê²Œ ì¤Œ (ì¡°ê¸°ì¢…ë£Œ ì—†ìŒ)
    """
    # Pool (ë²”ì£¼í˜• ì¸ë±ìŠ¤ ë°˜ì˜)
    pool_tr = Pool(Xtr, y_tr, cat_features=cat_cols_idx or None)
    pool_va = Pool(Xva, y_va, cat_features=cat_cols_idx or None)

    params = {
        "loss_function": "Logloss",
        "eval_metric": "AUC",           # ê¶¤ì ì„ AUC ê¸°ì¤€ìœ¼ë¡œ ë°›ìŒ
        "depth": best_cat_params.get("depth", 8),
        "learning_rate": best_cat_params.get("learning_rate", 0.05),
        "iterations": int(n_estimators_big),
        "random_seed": best_cat_params.get("random_seed", 2025),
        "verbose": False,
    }
    if "class_weights" in best_cat_params:
        params["class_weights"] = best_cat_params["class_weights"]

    model = CatBoostClassifier(**params)
    model.fit(pool_tr, eval_set=pool_va, use_best_model=False, verbose=False)
    ev = model.get_evals_result()
    # CatBoostëŠ” í‚¤ê°€ 'validation' ë˜ëŠ” 'learn'ìœ¼ë¡œ ìž¡íž˜ (ë²„ì „ì— ë”°ë¼ 'Validation'ì¼ ìˆ˜ë„ ìžˆì–´ ë³´ì •)
    key_candidates = ["validation", "Validation", "valid"]
    mkey = next((k for k in key_candidates if k in ev), None)
    if mkey is None:
        raise RuntimeError(f"CatBoost evals_resultì— validation í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: keys={list(ev.keys())}")

    auc_list = ev[mkey]["AUC"]
    iters = np.arange(1, len(auc_list)+1)
    val_error = 1.0 - np.array(auc_list, dtype=float)

    plt.figure(figsize=(7, 4.2))
    plt.plot(iters, val_error, linewidth=1.5)
    cw = best_cat_params.get("class_weights", None)
    cw_str = f", cw={cw}" if cw is not None else ""
    plt.title(f"CatBoost Error Trajectory | depth={params['depth']}, lr={params['learning_rate']}{cw_str}")
    plt.xlabel("iterations")
    plt.ylabel("Validation Error (1 - AUC)")
    plt.grid(True, linewidth=0.3)
    plt.savefig(RESULTS_DIR / f"cat_error_trajectory_{SEED}.png")
    plt.close()

    best_idx = int(np.argmin(val_error))
    logging.info(f"ðŸ”Ž [CAT] ìµœì†Œ ì—ëŸ¬ ì§€ì : iter={best_idx+1:,} | 1-AUC={val_error[best_idx]:.6f} | AUC={1 - val_error[best_idx]:.6f}")

def run_stage1_for_seeds(seeds=SEEDS, trials=50, do_plots=False, is_test_mode=False):
    results = []

    # âœ… í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° Optuna trial íšŸìˆ˜ ì¡°ì •
    if is_test_mode:
        logging.info("ðŸ”¥ Test mode: Optuna trials reduced to 3.")
        trials = 3

    # --- âœ… ì¶”ê°€ëœ ë¶€ë¶„: ì´ë¯¸ ì™„ë£Œëœ ì‹œë“œ ê±´ë„ˆë›°ê¸° ---
    completed_seeds = set()
    for f in RESULTS_DIR.glob("seed_*.joblib"):
        try:
            # íŒŒì¼ëª…ì—ì„œ ì‹œë“œ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: seed_2021.joblib -> 2021)
            completed_seeds.add(int(f.stem.split('_')[1]))
        except (ValueError, IndexError):
            continue

    logging.info(f"â–¶ï¸ Found {len(completed_seeds)} completed seeds: {sorted(list(completed_seeds))}")

    for sd in seeds:
        logging.info("="*70)

        # --- âœ… ì¶”ê°€ëœ ë¶€ë¶„: ì‹œë“œ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸ ---
        if sd in completed_seeds:
            logging.info(f"â–¶â–¶ Skipping seed {sd} (already completed). Loading from file.")
            res = joblib.load(RESULTS_DIR / f"seed_{sd}.joblib")
            results.append(res)
            continue
        # ---------------------------------------------

        logging.info(f"â–¶â–¶ Stage1 run for seed {sd}")
        res = run_stage1_only(trials=trials, seed=sd, is_test_mode=is_test_mode)

        # --- âœ… ì¶”ê°€ëœ ë¶€ë¶„: í˜„ìž¬ ì‹œë“œ ê²°ê³¼ ì €ìž¥ ---
        # ëª¨ë¸ ê°ì²´ ë•Œë¬¸ì— ìš©ëŸ‰ì´ í´ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, í•„ìš”í•œ ì •ë³´ë§Œ ì €ìž¥í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ì „ì²´ë¥¼ ì €ìž¥í•©ë‹ˆë‹¤.
        save_path = RESULTS_DIR / f"seed_{sd}.joblib"
        joblib.dump(res, save_path)
        logging.info(f"âœ… Saved results for seed {sd} to: {save_path}")
        # ---------------------------------------------

        results.append(res)

        if do_plots:
            best_lgb = res["models"]["lgbm"].get_params()
            plot_lgbm_error_trajectory(res["Xtr"], res["y_tr"], res["Xva"], res["y_va"], best_lgb, n_estimators_big=6000)
            best_xgb = res["models"]["xgb"].params
            plot_xgb_error_trajectory(res["Xtr"], res["y_tr"], res["Xva"], res["y_va"], best_xgb, n_estimators_big=6000)
            best_cat = res["models"]["cat"].get_params()
            plot_cat_error_trajectory(res["Xtr"], res["y_tr"], res["Xva"], res["y_va"], res["cat_cols_idx"], best_cat_params=best_cat, n_estimators_big=6000)

    # ìš”ì•½í‘œ
    summary_rows = []
    for res in results:
        best_params = res.get("best_params", {})
        row = {
            "seed":      int(res["seed"]),
            "F1":        float(res["F1"]),
            "Precision": float(res["Precision"]),
            "Recall":    float(res["Recall"]),
            "AUC":       float(res.get("AUC", float("nan"))),
            "strategy":  str(res["strategy"]),
            "best_lgbm": json.dumps(best_params.get("lgbm", {}), ensure_ascii=False),
            "best_xgb":  json.dumps(best_params.get("xgb",  {}), ensure_ascii=False),
            "best_cat":  json.dumps(best_params.get("cat",  {}), ensure_ascii=False),
            "cut_stage1": json.dumps(res.get("cutoffs", {}), ensure_ascii=False),
        }
        summary_rows.append(row)

    summary = pd.DataFrame(summary_rows).sort_values("seed")

    logging.info("\n===== Stage1 multi-seed summary (with AUC & Best Params) =====")
    logging.info(summary.to_string(index=False))

    summary_csv_path = RESULTS_DIR / "stage1_summary_results.csv"
    summary.to_csv(summary_csv_path, index=False)
    logging.info(f"âœ… Summary results (with AUC & Best Params) saved to: {summary_csv_path}")

    return {"by_seed": results, "summary": summary}


# =====================================================================================
# ---- 4. SCRIPT EXECUTION LOGIC
# =====================================================================================

def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(description="Run the LTV prediction pipeline.")
    parser.add_argument(
        '--stage',
        type=str,
        default='stage1',
        choices=['stage1', 'all'],
        help="Which part of the pipeline to run: 'stage1' for intermediate results, 'all' for the full pipeline."
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=DEFAULT_SEED,
        help=f"A specific random seed to run. Defaults to {DEFAULT_SEED}."
    )
    parser.add_argument(
        '--trials',
        type=int,
        default=50,
        help="Number of Optuna trials for Stage 1 hyperparameter tuning."
    )
    # âœ… í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì¸ìž ì¶”ê°€
    parser.add_argument(
        '--test_mode',
        action='store_true',  # ì´ ì˜µì…˜ì„ ì“°ë©´ Trueê°€ ë¨
        help="Run in test mode with sampled data and fewer trials for quick debugging."
    )

    # ë¨¼ì € íŒŒì‹±
    args = parser.parse_args()

    # ê·¸ ë‹¤ìŒì— test_mode ë¶„ê¸°
    if args.test_mode:
        logging.info("âš¡ Test mode: Narrowing search spaces and trials.")
        OPTUNA_TRIALS.update({"stage1": 3})
        # Stage1ìš© ë²”ìœ„ ì¶•ì†Œ
        global LGBM_N_EST_RANGE, XGB_N_EST_RANGE, CAT_ITER_RANGE
        LGBM_N_EST_RANGE = (50, 100)
        XGB_N_EST_RANGE  = (50, 100)
        CAT_ITER_RANGE   = (50, 100)

    # ==================================================================
    # âœ… ë¡œê±° ì„¤ì • (ìˆ˜ì •ë¨: ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€)
    # ==================================================================
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]
    log_file_path = f'ltv_pipeline_{timestamp}_{unique_id}.log'

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file_path),
            logging.StreamHandler(sys.stdout) # í„°ë¯¸ë„ í‘œì¤€ ì¶œë ¥ìœ¼ë¡œ ë³€ê²½
        ]
    )

    # âœ… ëª¨ë“  ì˜ˆì™¸(Exception)ë¥¼ ë¡œê·¸ íŒŒì¼ì— ê¸°ë¡í•˜ë„ë¡ ì„¤ì •
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        logging.error("ðŸ’¥ Uncaught exception", exc_info=(exc_type, exc_value, exc_traceback))

    sys.excepthook = handle_exception
    # =================================================================

    # ì´ì œ print ëŒ€ì‹  logging.infoë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    logging.info("=" * 80)
    logging.info(f"ðŸš€ Starting LTV Prediction Pipeline")
    logging.info(f"ðŸ”¹ Execution Stage: {args.stage}")
    logging.info(f"ðŸ”¹ Random Seed: {args.seed}")
    logging.info(f"ðŸ”¹ Data Directory: {DATA_DIR.resolve()}")
    logging.info(f"ðŸ”¹ Device: {DEVICE}")
    logging.info("=" * 80)

    if not DATA_DIR.exists():
        logging.info(f"âŒ FATAL: Data directory not found at {DATA_DIR}")
        logging.info("Please ensure your data is structured correctly under /data.")
        return

    # Set up environment
    pd.options.display.float_format = '{:,.2f}'.format
    warnings.filterwarnings("ignore")
    CPU = os.cpu_count() or 2
    os.environ["OMP_NUM_THREADS"] = str(CPU)
    
    try:
        if args.stage == 'stage1':
            # âœ… run_stage1_for_seeds í•¨ìˆ˜ë§Œ í˜¸ì¶œ
            run_stage1_for_seeds(seeds=SEEDS, trials=args.trials, is_test_mode=args.test_mode, do_plots=True)
        
        elif args.stage == 'all': # âœ… elifë¥¼ try ì•ˆìœ¼ë¡œ ì´ë™
            logging.info("\nâ–¶ï¸ Running the full pipeline across all seeds...")
            # agg_results = run_all_seeds(seeds=SEEDS, is_test_mode=args.test_mode) # test_mode ì „ë‹¬ ì¶”ê°€
            # final_evaluation = evaluate_after_seeds(agg_results)
            # logging.info("\nâœ… Full pipeline finished.")
            # logging.info(f"Final evaluation metrics: {final_evaluation['ensemble_mean']['metrics']}")
            logging.info("NOTE: 'all' stages execution logic needs to be fully implemented based on notebook.")
            
    except Exception as e: # âœ… try ë¸”ë¡ì´ ëë‚œ ì§í›„ì— exceptê°€ ì˜¤ë„ë¡ ìˆ˜ì •
        logging.error(f"â˜ ï¸ A critical error occurred during pipeline execution.")
        logging.error(traceback.format_exc()) # ì—ëŸ¬ ìƒì„¸ ë‚´ìš© ê¸°ë¡

if __name__ == "__main__":
    main()